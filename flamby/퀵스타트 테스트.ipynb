{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 4), (1, 5), (1, 6), (2, 4), (2, 5), (2, 6), (3, 4), (3, 5), (3, 6)]\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "epsilons = [1,2,3]\n",
    "deltas = [4,5,6]\n",
    "print(list(product(epsilons, deltas)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/dbswldms316/FLamby/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING USING DEBUG MODE DATASET EVEN THOUGH DEBUG WAS SET TO FALSE, COULD NOT FIND NON DEBUG DATASET CONFIG FILE\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "It seems the dataset fed_camelyon16 was not downloaded as the config file is not found for either normal or debug mode. Please refer to the download instructions inside FLamby/flamby/datasets/fed_camelyon16/README.md",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/FLamby/flamby/utils.py:221\u001b[0m, in \u001b[0;36mcheck_dataset_from_config\u001b[0;34m(dataset_name, debug)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mread_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_config_file_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "File \u001b[0;32m~/FLamby/flamby/utils.py:96\u001b[0m, in \u001b[0;36mread_config\u001b[0;34m(config_file)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(config_file)):\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find the config to read.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(config_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Could not find the config to read.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/FLamby/flamby/utils.py:237\u001b[0m, in \u001b[0;36mcheck_dataset_from_config\u001b[0;34m(dataset_name, debug)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 237\u001b[0m     \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mread_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_config_file_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "File \u001b[0;32m~/FLamby/flamby/utils.py:96\u001b[0m, in \u001b[0;36mread_config\u001b[0;34m(config_file)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(config_file)):\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find the config to read.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(config_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Could not find the config to read.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflamby\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfed_camelyon16\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FedCamelyon16, Camelyon16Raw\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Raw dataset\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m mydataset_raw \u001b[38;5;241m=\u001b[39m \u001b[43mCamelyon16Raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Pooled test dataset (전체 데이터셋 - centralized 일 때!)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m mydataset_pooled \u001b[38;5;241m=\u001b[39m FedCamelyon16(train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, pooled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/FLamby/flamby/datasets/fed_camelyon16/dataset.py:63\u001b[0m, in \u001b[0;36mCamelyon16Raw.__init__\u001b[0;34m(self, X_dtype, y_dtype, debug, data_path)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"See description above\"\"\"\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_dataset_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfed_camelyon16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtiles_dir \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_path\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/FLamby/flamby/utils.py:239\u001b[0m, in \u001b[0;36mcheck_dataset_from_config\u001b[0;34m(dataset_name, debug)\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m read_config(get_config_file_path(dataset_name, debug))\n\u001b[1;32m    238\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m--> 239\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    240\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt seems the dataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m was not downloaded as \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    241\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe config file is not found for either normal or debug \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode. Please refer to the download instructions inside \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFLamby/flamby/datasets/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/README.md\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    244\u001b[0m             )\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdownload_complete\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt seems the dataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m was only partially downloaded\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease restart the download script to finish the download.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: It seems the dataset fed_camelyon16 was not downloaded as the config file is not found for either normal or debug mode. Please refer to the download instructions inside FLamby/flamby/datasets/fed_camelyon16/README.md"
     ]
    }
   ],
   "source": [
    "from flamby.datasets.fed_camelyon16 import FedCamelyon16, Camelyon16Raw\n",
    "\n",
    "# Raw dataset\n",
    "mydataset_raw = Camelyon16Raw()\n",
    "\n",
    "# Pooled test dataset (전체 데이터셋 - centralized 일 때!)\n",
    "mydataset_pooled = FedCamelyon16(train=False, pooled=True)\n",
    "\n",
    "# Center 2 train dataset\n",
    "mydataset_local2= FedCamelyon16(center=2, train=True, pooled=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flamby.datasets.fed_isic2019.dataset import Isic2019Raw, FedIsic2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydataset_raw = Isic2019Raw()\n",
    "mydataset_pooled = FedIsic2019(train=True, pooled=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the first center as a pytorch dataset\n",
    "center0 = FedIsic2019(center=0, train=True)\n",
    "# To load the second center as a pytorch dataset\n",
    "center1 = FedIsic2019(center=1, train=True)\n",
    "# To load the 3rd center ...\n",
    "center2 = FedIsic2019(center=2, train=True)\n",
    "center3 = FedIsic2019(center=3, train=True)\n",
    "center4 = FedIsic2019(center=4, train=True)\n",
    "center5 = FedIsic2019(center=5, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flamby.datasets.fed_isic2019 import (\n",
    "    BATCH_SIZE,\n",
    "    LR,\n",
    "    NUM_EPOCHS_POOLED,\n",
    "    Baseline,\n",
    "    BaselineLoss,\n",
    "    metric,\n",
    "    NUM_CLIENTS,\n",
    "    Optimizer,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "Number of features output by EfficientNet 1280\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Instantiation of local train set (and data loader)), baseline loss function, baseline model, default optimizer\n",
    "train_dataset = FedIsic2019(center=1, train=True, pooled=False)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "lossfunc = BaselineLoss()\n",
    "model = Baseline()\n",
    "optimizer = Optimizer(model.parameters(), lr=LR)\n",
    "\n",
    "# Traditional pytorch training loop\n",
    "for epoch in range(0, NUM_EPOCHS_POOLED):\n",
    "    for idx, (X, y) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = lossfunc(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'client_test_0': 0.7765621995770045}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "from flamby.utils import evaluate_model_on_tests\n",
    "# Instantiation of a list of the local test sets\n",
    "test_dataloaders = [\n",
    "            torch.utils.data.DataLoader(\n",
    "                FedIsic2019(center=1, train=False, pooled=False),\n",
    "                batch_size=BATCH_SIZE,\n",
    "                shuffle=False,\n",
    "                num_workers=0,\n",
    "            )\n",
    "            #for i in range(NUM_CLIENTS)\n",
    "        ]\n",
    "# Function performing the evaluation\n",
    "dict_cindex = evaluate_model_on_tests(model, test_dataloaders, metric)\n",
    "print(dict_cindex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과 : {'client_test_0': 0.7558902342192252, 'client_test_1': 0.41120404195774307, 'client_test_2': 0.6664539082124209, 'client_test_3': 0.435638777683362, 'client_test_4': 0.3176664605236034, 'client_test_5': 0.5899024024024024}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "Number of features output by EfficientNet 1280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [16:58<00:00, 113.22s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from flamby.utils import evaluate_model_on_tests\n",
    "\n",
    "# 2 lines of code to change to switch to another dataset\n",
    "from flamby.datasets.fed_isic2019 import (\n",
    "    BATCH_SIZE,\n",
    "    LR,\n",
    "    NUM_EPOCHS_POOLED,\n",
    "    Baseline,\n",
    "    BaselineLoss,\n",
    "    metric,\n",
    "    NUM_CLIENTS,\n",
    "    get_nb_max_rounds\n",
    ")\n",
    "from flamby.datasets.fed_isic2019 import FedIsic2019\n",
    "\n",
    "# 1st line of code to change to switch to another strategy\n",
    "from flamby.strategies.fed_avg import FedAvg as strat\n",
    "\n",
    "# We loop on all the clients of the distributed dataset and instantiate associated data loaders\n",
    "train_dataloaders = [\n",
    "            torch.utils.data.DataLoader(\n",
    "                FedIsic2019(center = i, train = True, pooled = False),\n",
    "                batch_size = BATCH_SIZE,\n",
    "                shuffle = True,\n",
    "                num_workers = 0\n",
    "            )\n",
    "            for i in range(NUM_CLIENTS)\n",
    "        ]\n",
    "\n",
    "lossfunc = BaselineLoss()\n",
    "m = Baseline()\n",
    "\n",
    "# Federated Learning loop\n",
    "# 2nd line of code to change to switch to another strategy (feed the FL strategy the right HPs)\n",
    "args = {\n",
    "            \"training_dataloaders\": train_dataloaders,\n",
    "            \"model\": m,\n",
    "            \"loss\": lossfunc,\n",
    "            \"optimizer_class\": torch.optim.SGD,\n",
    "            \"learning_rate\": LR / 10.0,\n",
    "            \"num_updates\": 100,\n",
    "# This helper function returns the number of rounds necessary to perform approximately as many\n",
    "# epochs on each local dataset as with the pooled training\n",
    "            \"nrounds\": get_nb_max_rounds(100),\n",
    "        }\n",
    "s = strat(**args)\n",
    "m = s.run()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:34<00:00,  5.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'client_test_0': 0.2511727121732762, 'client_test_1': 0.2511727121732762, 'client_test_2': 0.2511727121732762, 'client_test_3': 0.2511727121732762, 'client_test_4': 0.2511727121732762, 'client_test_5': 0.2511727121732762}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluation\n",
    "# We only instantiate one test set in this particular case: the pooled one\n",
    "test_dataloaders = [\n",
    "            torch.utils.data.DataLoader(\n",
    "                FedIsic2019(center=i, train = False, pooled = True),\n",
    "                batch_size = BATCH_SIZE,\n",
    "                shuffle = False,\n",
    "                num_workers = 0,\n",
    "            )\n",
    "            for i in range(NUM_CLIENTS)\n",
    "\n",
    "        ]\n",
    "dict_cindex = evaluate_model_on_tests(m, test_dataloaders, metric)\n",
    "print(dict_cindex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과 : {'client_test_0': 0.2784241354353424, 'client_test_1': 0.2784241354353424, 'client_test_2': 0.2784241354353424, 'client_test_3': 0.2784241354353424, 'client_test_4': 0.2784241354353424, 'client_test_5': 0.2784241354353424}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.15.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard)\n",
      "  Downloading grpcio-1.60.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard)\n",
      "  Downloading google_auth-2.26.2-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard)\n",
      "  Using cached google_auth_oauthlib-1.2.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard)\n",
      "  Downloading Markdown-3.5.2-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/dbswldms316/miniconda3/envs/flam/lib/python3.9/site-packages (from tensorboard) (1.26.3)\n",
      "Collecting protobuf<4.24,>=3.19.6 (from tensorboard)\n",
      "  Downloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl.metadata (540 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/dbswldms316/miniconda3/envs/flam/lib/python3.9/site-packages (from tensorboard) (2.31.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/dbswldms316/miniconda3/envs/flam/lib/python3.9/site-packages (from tensorboard) (68.2.2)\n",
      "Requirement already satisfied: six>1.9 in /home/dbswldms316/miniconda3/envs/flam/lib/python3.9/site-packages (from tensorboard) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard)\n",
      "  Downloading werkzeug-3.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard)\n",
      "  Using cached cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard)\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard)\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<2,>=0.5->tensorboard)\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4 (from markdown>=2.6.8->tensorboard)\n",
      "  Downloading importlib_metadata-7.0.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/dbswldms316/miniconda3/envs/flam/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dbswldms316/miniconda3/envs/flam/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/dbswldms316/miniconda3/envs/flam/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dbswldms316/miniconda3/envs/flam/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/dbswldms316/miniconda3/envs/flam/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/dbswldms316/miniconda3/envs/flam/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.17.0)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard)\n",
      "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth-2.26.2-py2.py3-none-any.whl (186 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.5/186.5 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading grpcio-1.60.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading Markdown-3.5.2-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.9/103.9 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Downloading importlib_metadata-7.0.1-py3-none-any.whl (23 kB)\n",
      "Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: werkzeug, tensorboard-data-server, pyasn1, protobuf, oauthlib, importlib-metadata, grpcio, cachetools, absl-py, rsa, requests-oauthlib, pyasn1-modules, markdown, google-auth, google-auth-oauthlib, tensorboard\n",
      "Successfully installed absl-py-2.1.0 cachetools-5.3.2 google-auth-2.26.2 google-auth-oauthlib-1.2.0 grpcio-1.60.0 importlib-metadata-7.0.1 markdown-3.5.2 oauthlib-3.2.2 protobuf-4.23.4 pyasn1-0.5.1 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.15.1 tensorboard-data-server-0.7.2 werkzeug-3.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FedHeartDisease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flamby.datasets.fed_heart_disease import FedHeartDisease, HeartDiseaseRaw\n",
    "\n",
    "# To load the first center as a pytorch dataset\n",
    "center0 = FedHeartDisease(center=0, train=True)\n",
    "# To load the second center as a pytorch dataset\n",
    "center1 = FedHeartDisease(center=1, train=True)\n",
    "# To sample batches from each of the local datasets use the traditional pytorch API\n",
    "from torch.utils.data import DataLoader as dl\n",
    "\n",
    "X, y = iter(dl(center0, batch_size=16, shuffle=True, num_workers=0)).__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 399.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'client_test_0': 0.7788461538461539, 'client_test_1': 0.7752808988764045, 'client_test_2': 0.75, 'client_test_3': 0.6444444444444445}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from flamby.utils import evaluate_model_on_tests\n",
    "\n",
    "# 2 lines of code to change to switch to another dataset\n",
    "from flamby.datasets.fed_heart_disease import (\n",
    "    BATCH_SIZE,\n",
    "    LR,\n",
    "    NUM_EPOCHS_POOLED,\n",
    "    Baseline,\n",
    "    BaselineLoss,\n",
    "    metric,\n",
    "    NUM_CLIENTS,\n",
    "    Optimizer,\n",
    ")\n",
    "from flamby.datasets.fed_heart_disease import FedHeartDisease as FedDataset\n",
    "\n",
    "# Instantiation of local train set (and data loader)), baseline loss function, baseline model, default optimizer\n",
    "train_dataset = FedDataset(center=0, train=True, pooled=False)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "lossfunc = BaselineLoss()\n",
    "model = Baseline()\n",
    "optimizer = Optimizer(model.parameters(), lr=LR)\n",
    "\n",
    "# Traditional pytorch training loop\n",
    "for epoch in range(0, NUM_EPOCHS_POOLED):\n",
    "    for idx, (X, y) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = lossfunc(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "# Instantiation of a list of the local test sets\n",
    "test_dataloaders = [\n",
    "            torch.utils.data.DataLoader(\n",
    "                FedDataset(center=i, train=False, pooled=False),\n",
    "                batch_size=BATCH_SIZE,\n",
    "                shuffle=False,\n",
    "                num_workers=0,\n",
    "            )\n",
    "            for i in range(NUM_CLIENTS)\n",
    "        ]\n",
    "# Function performing the evaluation\n",
    "dict_cindex = evaluate_model_on_tests(model, test_dataloaders, metric)\n",
    "print(dict_cindex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fed-TCGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving license agreement\n",
      "You may now proceed to download.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from flamby.datasets.fed_tcga_brca import TcgaBrcaRaw, FedTcgaBrca\n",
    "\n",
    "# Raw dataset\n",
    "mydataset_raw = TcgaBrcaRaw()\n",
    "\n",
    "# Pooled test dataset\n",
    "mydataset_pooled = FedTcgaBrca(train=False, pooled=True)\n",
    "\n",
    "# Center 2 train dataset\n",
    "mydataset_local2= FedTcgaBrca(center=2, train=True, pooled=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dbswldms316/FLamby/flamby/datasets/fed_tcga_brca/dataset.py:51: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  return (torch.tensor(x, dtype=self.X_dtype), torch.tensor(y, dtype=self.y_dtype))\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]/home/dbswldms316/FLamby/flamby/datasets/fed_tcga_brca/dataset.py:51: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  return (torch.tensor(x, dtype=self.X_dtype), torch.tensor(y, dtype=self.y_dtype))\n",
      "100%|██████████| 6/6 [00:01<00:00,  5.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'client_test_0': 0.664804469273743, 'client_test_1': 0.5681818181818182, 'client_test_2': 0.7685185185185185, 'client_test_3': 0.5416666666666666, 'client_test_4': 0.7021276595744681, 'client_test_5': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from flamby.utils import evaluate_model_on_tests\n",
    "\n",
    "# 2 lines of code to change to switch to another dataset\n",
    "from flamby.datasets.fed_tcga_brca import (\n",
    "    BATCH_SIZE,\n",
    "    LR,\n",
    "    NUM_EPOCHS_POOLED,\n",
    "    Baseline,\n",
    "    BaselineLoss,\n",
    "    metric,\n",
    "    NUM_CLIENTS,\n",
    "    Optimizer,\n",
    ")\n",
    "from flamby.datasets.fed_tcga_brca import FedTcgaBrca as FedDataset\n",
    "\n",
    "# Instantiation of local train set (and data loader)), baseline loss function, baseline model, default optimizer\n",
    "train_dataset = FedDataset(center=0, train=True, pooled=False)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "lossfunc = BaselineLoss()\n",
    "model = Baseline()\n",
    "optimizer = Optimizer(model.parameters(), lr=LR)\n",
    "\n",
    "# Traditional pytorch training loop\n",
    "for epoch in range(0, NUM_EPOCHS_POOLED):\n",
    "    for idx, (X, y) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = lossfunc(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "# Instantiation of a list of the local test sets\n",
    "test_dataloaders = [\n",
    "            torch.utils.data.DataLoader(\n",
    "                FedDataset(center=i, train=False, pooled=False),\n",
    "                batch_size=BATCH_SIZE,\n",
    "                shuffle=False,\n",
    "                num_workers=0,\n",
    "            )\n",
    "            for i in range(NUM_CLIENTS)\n",
    "        ]\n",
    "# Function performing the evaluation\n",
    "dict_cindex = evaluate_model_on_tests(model, test_dataloaders, metric)\n",
    "print(dict_cindex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]/home/dbswldms316/FLamby/flamby/datasets/fed_tcga_brca/dataset.py:51: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  return (torch.tensor(x, dtype=self.X_dtype), torch.tensor(y, dtype=self.y_dtype))\n",
      "100%|██████████| 5/5 [00:27<00:00,  5.55s/it]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]/home/dbswldms316/FLamby/flamby/datasets/fed_tcga_brca/dataset.py:51: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  return (torch.tensor(x, dtype=self.X_dtype), torch.tensor(y, dtype=self.y_dtype))\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'client_test_0': 0.7688644688644689, 'client_test_1': 0.7688644688644689, 'client_test_2': 0.7688644688644689, 'client_test_3': 0.7688644688644689, 'client_test_4': 0.7688644688644689, 'client_test_5': 0.7688644688644689}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from flamby.utils import evaluate_model_on_tests\n",
    "\n",
    "# 2 lines of code to change to switch to another dataset\n",
    "from flamby.datasets.fed_tcga_brca import (\n",
    "    BATCH_SIZE,\n",
    "    LR,\n",
    "    NUM_EPOCHS_POOLED,\n",
    "    Baseline,\n",
    "    BaselineLoss,\n",
    "    metric,\n",
    "    NUM_CLIENTS,\n",
    "    get_nb_max_rounds\n",
    ")\n",
    "from flamby.datasets.fed_tcga_brca import FedTcgaBrca as FedDataset\n",
    "\n",
    "# 1st line of code to change to switch to another strategy\n",
    "from flamby.strategies.fed_avg import FedAvg as strat\n",
    "\n",
    "# We loop on all the clients of the distributed dataset and instantiate associated data loaders\n",
    "train_dataloaders = [\n",
    "            torch.utils.data.DataLoader(\n",
    "                FedDataset(center = i, train = True, pooled = False),\n",
    "                batch_size = BATCH_SIZE,\n",
    "                shuffle = True,\n",
    "                num_workers = 0\n",
    "            )\n",
    "            for i in range(NUM_CLIENTS)\n",
    "        ]\n",
    "\n",
    "lossfunc = BaselineLoss()\n",
    "m = Baseline()\n",
    "\n",
    "# Federated Learning loop\n",
    "# 2nd line of code to change to switch to another strategy (feed the FL strategy the right HPs)\n",
    "args = {\n",
    "            \"training_dataloaders\": train_dataloaders,\n",
    "            \"model\": m,\n",
    "            \"loss\": lossfunc,\n",
    "            \"optimizer_class\": torch.optim.SGD,\n",
    "            \"learning_rate\": LR / 10.0,\n",
    "            \"num_updates\": 100,\n",
    "# This helper function returns the number of rounds necessary to perform approximately as many\n",
    "# epochs on each local dataset as with the pooled training\n",
    "            \"nrounds\": get_nb_max_rounds(100),\n",
    "        }\n",
    "s = strat(**args)\n",
    "m = s.run()[0]\n",
    "\n",
    "# Evaluation\n",
    "# We only instantiate one test set in this particular case: the pooled one\n",
    "test_dataloaders = [\n",
    "            torch.utils.data.DataLoader(\n",
    "                FedDataset(center=i, train = False, pooled = True),\n",
    "                batch_size = BATCH_SIZE,\n",
    "                shuffle = False,\n",
    "                num_workers = 0,\n",
    "            )\n",
    "            for i in range(NUM_CLIENTS)\n",
    "        ]\n",
    "dict_cindex = evaluate_model_on_tests(m, test_dataloaders, metric)\n",
    "print(dict_cindex)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
